{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d37015c-06ea-4284-955b-4649d7f23eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f192ae5f-2bd6-4ddf-8596-85a95a3f9fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    MinMaxScaler,\n",
    "    OneHotEncoder,\n",
    "    PolynomialFeatures,\n",
    "    power_transform,\n",
    "    PowerTransformer,\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from preprocessing import numeric_object_split, z_filter\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc52134-27a8-4c29-a67a-3e03f23524f9",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to aggregate data on multiple different linear models. As interesting as it is to theorize the effects of feature engineering, scaling, imputing, etc. during EDA, ultimately what determines what the best actions are going to be is determined by model performance. Thus, this notebook will declare all options I wish to explore, iterate through all combinations of them, create a model with the corresponding hyperparameters, fit the data and return the $R^2$ and RMSE values. The parameters and scores are then saved to a file for use in model tuning. \n",
    "\n",
    "## Model Options\n",
    "\n",
    "The first option I wish to explore is imputation method. The first being a simple imputation of the mean value of the feature, and the second being a K-Nearest Neighbors imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1727d806-0d9d-4c58-8a70-c44becf06653",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPUTER_VARIANTS = {\n",
    "    \"mean\": SimpleImputer(strategy=\"mean\"),\n",
    "    \"knn\": KNNImputer(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d252ad-d9dd-4e1c-b712-4e717ca142cf",
   "metadata": {},
   "source": [
    "Next, there are the model versions. The first is the standard variant of linear regression, which is ordinary least squares. The next two are Lasso and Ridge, which include an $L_1$ and $L_2$ regularization penalty, respectively. Finally, ElasticNet models will be tested, which include both an $L_1$ *and* $L_2$ penalty. It is the ElasticNet model that I assume will form the best predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ce5858b-f8e0-4f13-8ed5-c92f727e2120",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDAS = [0.01] + list(np.linspace(0, 0.1, 5)[1:])\n",
    "\n",
    "REGRESSOR_VARIANTS = (\n",
    "    {\"ols\": LinearRegression()}\n",
    "    | {f\"lasso_{l1}\": Lasso(alpha=l1) for l1 in LAMBDAS}\n",
    "    | {f\"ridge_{l2}\": Lasso(alpha=l2) for l2 in LAMBDAS}\n",
    "    | {\n",
    "        f\"elasticnet_{l1}_{l2}\": ElasticNet(alpha=l1 + l2, l1_ratio=l1 / (l1 + l2))\n",
    "        for l1, l2 in itertools.product(LAMBDAS, LAMBDAS)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af76c7d2-cb9d-4ff1-aa39-dc486cbc65ef",
   "metadata": {},
   "source": [
    "The two versions of the target variable are available to fit, being the provided target of sale_price, and my constructed alternative target price_per_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c715703-1a42-4489-a859-1ef8186c4d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGETS = [\"saleprice\", \"price_per_area\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7ba092-146d-488b-bc97-c63fe6692733",
   "metadata": {},
   "source": [
    "Next, there are three different sets of features which I explored during EDA. The first is whether to use all features, or only those with numeric data types. While having more features to learn would in theory grant a better fit, it can also lead to overfitting, and thus limiting the model to only numerical data may increase performance on validation data. \n",
    "\n",
    "For area-like features, it was noted that gr_liv_area was the sum of the areas of the first and second floors. Whether to include total livable area or the independent floor 1 and 2 areas is something that the model will determine. There are three versions of the area set: total livable area only, floor 1 and 2 area only, and all included.\n",
    "\n",
    "For time, a similar approach was taken. The sale time was originally encoded in two columns, corresponding to the year and month. Thus one variant of the areas leaves uses these two columns and one-hot encodes them. The next uses a combined year-month string which is encoded. A third variant converts the date to a timestamp and fits on it. The fourth variant uses both the timestamp and year-month columns. Finally, a fifth variant uses all of the time columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb4382a4-4951-42ac-8df4-f38d5b79575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_train = pd.read_csv(\n",
    "    \"../datasets/train_preprocessed.csv\",\n",
    "    dtype={\"ms_subclass\": object, \"yr_sold\": object, \"mo_sold\": object},\n",
    ")\n",
    "\n",
    "numerics, objects = numeric_object_split(ames_train)\n",
    "\n",
    "FEATURE_SETS = {\n",
    "    # name: columns to drop\n",
    "    \"full\": [],\n",
    "    \"numeric\": objects.columns,\n",
    "}\n",
    "AREA_SETS = {\n",
    "    \"full\": [],\n",
    "    \"total_grade_area\": [\"1st_flr_sf\", \"2nd_flr_sf\"],\n",
    "    \"separate_floors\": [\"gr_liv_area\"],\n",
    "}\n",
    "TIME_SETS = {\n",
    "    \"full\": [],\n",
    "    \"timestamp\": [\"year_month\", \"yr_sold\", \"mo_sold\"],\n",
    "    \"year_month\": [\"sale_timestamp\", \"yr_sold\", \"mo_sold\"],\n",
    "    \"year_and_month\": [\"sale_timestamp\", \"year_month\"],\n",
    "    \"timestamp_and_year_month\": [\"yr_sold\", \"mo_sold\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fb79b4-c8bc-4165-8806-4e19b354a310",
   "metadata": {},
   "source": [
    "One of the things I will attempt to implement in this model is data splitting: breaking the data into logical subsets, and fitting each subset with its own regression model in a matter similar to a regression tree. EDA of the training data made two logical splits apparent to me. The first was for homes which had a pid starting with \"5\" or \"9\", as the groups were fairly even. The second was to split on if a home had an \"average\" overall condition (5). Homes with conditions *other* than 5 seemed to have their price increase linearly with the variable, but there were so many homes with an average condition that the overall simple fit was incredibly weak. As such, it made sense to me to analyze average homes and non-average homes as two separate groups. These two splitting methods define four split sets: neither, pid only, condition only, and both. Additionally, a function is declared here that will construct a set of indices to be included in each split so that the data can be easily decomposed into subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fcfbaff-0a6d-4b23-b484-e8c889551abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_SET = {\n",
    "    # name: (column, value) to split on\n",
    "    \"no_split\": [],\n",
    "    \"pid_split\": [(\"pid_5\", 1)],\n",
    "    \"average_condition_split\": [(\"overall_cond\", 5)],\n",
    "    \"pid_condition_split\": [(\"pid_5\", 1), (\"overall_cond\", 5)],\n",
    "}\n",
    "\n",
    "\n",
    "def splitter(split_index, size):\n",
    "    # given a number of splits to perform\n",
    "    # this function will return a list of\n",
    "    # boolean indexers to apply to\n",
    "    # the data\n",
    "    # if split index is empty, returns a single full split\n",
    "    # in general returns 2 ** num_splitters splits\n",
    "    num_splitters = len(split_index)\n",
    "\n",
    "    num_arrays = 2**num_splitters\n",
    "    truth_arrays = np.array([[True for _ in range(size)] for n in range(num_arrays)])\n",
    "\n",
    "    binary = [True, False]\n",
    "\n",
    "    for i, combo in enumerate(itertools.product(binary, repeat=num_splitters)):\n",
    "        for j, boolean in enumerate(combo):\n",
    "            if boolean:\n",
    "                truth_arrays[i] *= split_index[j]\n",
    "            else:\n",
    "                truth_arrays[i] *= np.logical_not(split_index[j])\n",
    "\n",
    "    return truth_arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65777325-91fe-41c7-8b1d-d7aa259a3a1f",
   "metadata": {},
   "source": [
    "Finally, we define a few thresholds for the data. The z threshold is the maximum value of the absolute value of the z-scored data to be included for fitting. This is to prevent outliers from having too much of an effect on the regression process. The covariance threshold is the maximum amount of the absolute value of covariance any two standardized features are allowed to share. This is a systematic way to remove colinear variables from the dataset prior to regression. Both z threshold and covariance threshold were only given a single value to reduce the analysis time, as this process is already quite lengthy.\n",
    "\n",
    "Target covariance threshold represents the minimum amount of correlation a feature must have with the target. Larger values will filter out the weaker features, leaving only a few well-fitting columns to be used in regression. This will help to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4d280a6-71da-4b1e-b2b6-99ccd3d59fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "COVARIANCE_THRESHOLD = 0.9\n",
    "TARGET_COVARIANCE_THRESHOLDS = [0.0, 0.2, 0.4]\n",
    "Z_THRESHOLD = 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988e79a8-4645-4fde-a1ef-eb803ad564c8",
   "metadata": {},
   "source": [
    "Here, the total number of iterations is calculated (51840) to give an estimate of progress during the grid search. As my machine is fairly powerful, this will take only a few hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf951f7b-ab75-42f1-a9b5-bab9cbdfc76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_iterations = (\n",
    "    len(IMPUTER_VARIANTS)\n",
    "    * len(TARGET_COVARIANCE_THRESHOLDS)\n",
    "    * len(TARGETS)\n",
    "    * len(SPLIT_SET)\n",
    "    * len(FEATURE_SETS)\n",
    "    * len(TIME_SETS)\n",
    "    * len(AREA_SETS)\n",
    "    * len(REGRESSOR_VARIANTS)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5102e095-4c11-4a2e-afa0-55d641b013d8",
   "metadata": {},
   "source": [
    "Additionally, I declare two functions to be used during preprocessing. The first is rather comical, and exists to *reintroduce* null values to a dataset. This is due to the way that sklearn handles null values during one-hot encoding, which is to create their own feature. I didn't like this, and thought that examples with null values should be left null for a proper imputer to handle, but this doesn't seem to be an option sklearn's OneHotEncoder provides. Therefore, I chose instead to provide the encoder with a list of categories to explicitly create features, then checked if *all* relevant columns for the original feature were zero. If they were, the zeros were replaced with np.nan. The reason I didn't impute *before* encoding is so that I could make use of a KNN imputer, which requires properly-scaled, numeric features. Thus the order of operations was forced to be scale -> encode -> put nulls back -> impute.\n",
    "\n",
    "The second function is to check if, after other filtering methods have been applied, any features are left with only a single value. As these features now have zero variance, this can cause issues during computation and must be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24dc212a-4cf4-47c8-a3db-89165aa3b2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def renanner(X, categories):\n",
    "    # reintroduce nans to the dataset (lol)\n",
    "    col = 0\n",
    "    for cat in categories:\n",
    "        end_col = col + len(cat)\n",
    "        subset = X[:, col:end_col]\n",
    "        for n, row in enumerate(subset):\n",
    "            if np.all(row == 0):\n",
    "                new_row = np.array([np.nan for i in range(len(row))])\n",
    "                X[n, col:end_col] = new_row\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def homogenous_columns(X):\n",
    "    unique_counts = np.array(\n",
    "        [len(u) for u in [np.unique(X[:, col]) for col in range(X.shape[1])]]\n",
    "    )\n",
    "    return unique_counts == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ad8a07-18aa-4716-882d-6ff73a932ea5",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "The following code cell is large and dense, so I will outline its logic here in addition to comments. For each step in the loop below:\n",
    "\n",
    "1. The relevant features are selected, scaled, encoded, and have their missing values imputed.\n",
    "\n",
    "2. Features with high bivariate correlation and/or low target correlation are systematically dropped. Which features are dropped are *not* tracked here, but will be in the model tuning phase.\n",
    "\n",
    "3. Features are transformed via a Yeo-Johnson transformation. The transformation parameter is not tracked.\n",
    "\n",
    "4. Data is split into subsets and fit with a linear model. Polynomial features are not tested here due to memory constraints.\n",
    "\n",
    "5. Model parameters and scores are written to a file\n",
    "\n",
    "The \"best\" model as determined by cross-validation score is continuously displayed as the output of the cell, as well as a few progress metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d3975e5-4664-4278-9914-0372d375416f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations completed: 51840/51840\n",
      "Elapsed time: 0:58:27\n",
      "Estimated time remaining: 0:00:00\n",
      "\n",
      "Best cross-validation score: 0.91338\n",
      "Best RMSE: 16859.357\n",
      "\n",
      "Regressor: lasso_0.01\n",
      "Imputer: mean\n",
      "Feature set: full\n",
      "Area set: full\n",
      "Time set: full\n",
      "Split: no_split\n",
      "Covariance threshold: 0.0\n",
      "Target: saleprice\n"
     ]
    }
   ],
   "source": [
    "# a csv file is created to store model hyperparameters and scores\n",
    "with open(\n",
    "    f\"../etc/model_results_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}.csv\", \"w+\"\n",
    ") as log:\n",
    "    log.write(\n",
    "        \"regressor,imputer,feature_set,area_set,time_set,split_name,covariance_threshold,target,train_score,val_score,cross_val_score,train_rmse,val_rmse\\n\"\n",
    "    )\n",
    "    # training data is z filtered\n",
    "    ames_train = z_filter(ames_train, high=Z_THRESHOLD, low=-Z_THRESHOLD)\n",
    "\n",
    "    # a few dummy values are declared for metrics I will be tracking\n",
    "    start_time = time.time()\n",
    "    iterations_completed = 0\n",
    "    best_cross_val_score = 0\n",
    "    best_rmse = 0\n",
    "    best_config = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "    # feature sets are looped through\n",
    "    for feature_set_name, feature_set in FEATURE_SETS.items():\n",
    "\n",
    "        # time sets are looped through\n",
    "        for time_set_name, time_set in TIME_SETS.items():\n",
    "\n",
    "            # area sets are looped through\n",
    "            for area_set_name, area_set in AREA_SETS.items():\n",
    "\n",
    "                # target variants are looped through\n",
    "                for target in TARGETS:\n",
    "                    # target values are declared and manually standardized\n",
    "                    # the mean and standard dev are stored for destanadardizing\n",
    "                    # this will ensure RMSE is interpretable\n",
    "                    y_full = ames_train[target]\n",
    "                    y_mean = y_full.mean()\n",
    "                    y_std = y_full.std()\n",
    "                    y = np.array((y_full - y_mean) / y_std)\n",
    "\n",
    "                    # features are dropped based on the feature, time, and area set\n",
    "                    features_to_drop = list(\n",
    "                        np.unique(np.concatenate([feature_set, time_set, area_set]))\n",
    "                    )\n",
    "\n",
    "                    # a \"transformer\" is defined to transform the target to sale price\n",
    "                    # if target is sale price, the tranformer is identity\n",
    "                    # otherwise, it is surface area\n",
    "                    x_transform = np.ones(y.shape[0])\n",
    "                    if target == \"price_per_area\":\n",
    "                        features_to_drop.append(\"gr_liv_area\")\n",
    "                        features_to_drop.append(\"1st_flr_sf\")\n",
    "                        features_to_drop.append(\"2nd_flr_sf\")\n",
    "\n",
    "                        x_transform = np.array(ames_train[\"gr_liv_area\"])\n",
    "\n",
    "                    # the full array of features is formally decalered here\n",
    "                    X_full = ames_train[\n",
    "                        [\n",
    "                            col\n",
    "                            for col in ames_train.columns\n",
    "                            if col not in features_to_drop and col not in TARGETS\n",
    "                        ]\n",
    "                    ]\n",
    "\n",
    "                    # it is easiest to define split conditions\n",
    "                    # on unscaled data so the split mask arrays\n",
    "                    # are created here\n",
    "                    # the data is not yet split\n",
    "                    split_indices = {}\n",
    "                    for name, split in SPLIT_SET.items():\n",
    "                        split_indices[name] = []\n",
    "                        for col, value in split:\n",
    "                            split_indices[name].append(X_full[col] == value)\n",
    "\n",
    "                    # the dataframe is broken into numeric and categorical subsets\n",
    "                    X_numerics, X_objects = numeric_object_split(X_full)\n",
    "\n",
    "                    # numeric values are scaled\n",
    "                    scaler = StandardScaler()\n",
    "                    scaled = scaler.fit_transform(X_numerics)\n",
    "\n",
    "                    # expected categories are declared\n",
    "                    categories = [X_objects[col].unique() for col in X_objects.columns]\n",
    "                    categories = [cat[[c != \"nan\" for c in cat]] for cat in categories]\n",
    "\n",
    "                    # categorical features are one-hot encoded\n",
    "                    ohe = OneHotEncoder(categories=categories, sparse=False)\n",
    "                    encoded = ohe.fit_transform(X_objects)\n",
    "                    # null values are reintroduced for proper imputation\n",
    "                    encoded = renanner(encoded, categories)\n",
    "\n",
    "                    # the formatted numeric and object columns are recombined\n",
    "                    X_full = np.concatenate([scaled, encoded], axis=1)\n",
    "\n",
    "                    # imputer variants are looped through\n",
    "                    for imputer_name, imputer in IMPUTER_VARIANTS.items():\n",
    "                        # null values are imputed\n",
    "                        imputed = imputer.fit_transform(X_full)\n",
    "\n",
    "                        # drop columns with only a single value\n",
    "                        imputed = imputed[:, ~homogenous_columns(imputed)]\n",
    "\n",
    "                        # this part's cool\n",
    "                        # bivariate correlations are computed for all features\n",
    "                        # each feature is treated as a node on a graph\n",
    "                        # features with strong colinearity have an edge drawn between their nodes\n",
    "                        corr = np.corrcoef(imputed, rowvar=False)\n",
    "                        correlation_graph = nx.Graph()\n",
    "                        correlation_graph.add_nodes_from(range(corr.shape[0]))\n",
    "                        for row in range(corr.shape[0]):\n",
    "                            high_bivariate_corr = np.where(\n",
    "                                np.abs(corr[row]) >= COVARIANCE_THRESHOLD\n",
    "                            )[0]\n",
    "                            for idx in high_bivariate_corr:\n",
    "                                correlation_graph.add_edge(row, idx)\n",
    "                        correlation_graph.remove_edges_from(\n",
    "                            nx.selfloop_edges(correlation_graph)\n",
    "                        )\n",
    "\n",
    "                        # once the graph is completed, nodes with edges are randomly removed\n",
    "                        # the node is drawn from the set of nodes with the highest number of edges\n",
    "                        # this process is repeated until the graph is unconnected\n",
    "                        # this leaves only nodes with *acceptable* levels of colinearity\n",
    "                        # the remaining nodes are then used to index the full dataset\n",
    "                        max_degree = 1\n",
    "                        while max_degree > 0:\n",
    "                            degree = correlation_graph.degree()\n",
    "                            max_degree = np.max([d for n, d in degree])\n",
    "                            worst_nodes = [n for n, d in degree if d == max_degree]\n",
    "                            correlation_graph.remove_node(np.random.choice(worst_nodes))\n",
    "\n",
    "                        features_to_keep = correlation_graph.nodes\n",
    "\n",
    "                        X_no_bivariate_corr = imputed[:, features_to_keep]\n",
    "\n",
    "                        # target covariance thresholds are iterated over\n",
    "                        # we note here that because the data is standardized\n",
    "                        # cov and corr are interchangable values\n",
    "                        for target_covariance_threshold in TARGET_COVARIANCE_THRESHOLDS:\n",
    "                            # feature target correlations are calculated\n",
    "                            target_corr = np.array(\n",
    "                                [\n",
    "                                    np.corrcoef(y_full, x)[0, 1]\n",
    "                                    for x in X_no_bivariate_corr.T\n",
    "                                ]\n",
    "                            )\n",
    "\n",
    "                            # low correlation features are dropped\n",
    "                            features_to_keep = np.where(\n",
    "                                np.abs(target_corr) >= target_covariance_threshold\n",
    "                            )[0]\n",
    "\n",
    "                            # a yeo-johnson transform is applied to the features\n",
    "                            X = power_transform(\n",
    "                                X_no_bivariate_corr[:, features_to_keep]\n",
    "                            )\n",
    "\n",
    "                            # regressor versions are looped through\n",
    "                            for regressor_name, regressor in REGRESSOR_VARIANTS.items():\n",
    "\n",
    "                                # split methods are looped through\n",
    "                                for split_name, split_index in split_indices.items():\n",
    "\n",
    "                                    # empty variables are declared to track\n",
    "                                    # which examples go into which split\n",
    "                                    # so that the data can be properly reconstructed\n",
    "                                    y_preds = np.zeros(y.shape[0])\n",
    "\n",
    "                                    indices = np.arange(y.shape[0])\n",
    "\n",
    "                                    train_indices, val_indices = np.array(\n",
    "                                        [], dtype=int\n",
    "                                    ), np.array([], dtype=int)\n",
    "\n",
    "                                    # cross val score is taken as a weighted average\n",
    "                                    # of cross val scores of individual splits\n",
    "                                    cross_scores = []\n",
    "                                    split_sizes = []\n",
    "\n",
    "                                    # split indexing arrays are declared\n",
    "                                    splits = splitter(split_index, X.shape[0])\n",
    "\n",
    "                                    # data subsets are looped through\n",
    "                                    for split in splits:\n",
    "\n",
    "                                        # train test splits are created\n",
    "                                        (\n",
    "                                            X_train,\n",
    "                                            X_val,\n",
    "                                            y_train,\n",
    "                                            y_val,\n",
    "                                            indices_train,\n",
    "                                            indices_val,\n",
    "                                        ) = train_test_split(\n",
    "                                            X[split],\n",
    "                                            y[split],\n",
    "                                            indices[split],\n",
    "                                            train_size=0.7,\n",
    "                                            random_state=42,\n",
    "                                        )\n",
    "\n",
    "                                        # example indices are tracked\n",
    "                                        (\n",
    "                                            indices_train,\n",
    "                                            indices_val,\n",
    "                                        ) = indices_train.astype(\n",
    "                                            int\n",
    "                                        ), indices_val.astype(\n",
    "                                            int\n",
    "                                        )\n",
    "\n",
    "                                        train_indices = np.append(\n",
    "                                            train_indices, indices_train\n",
    "                                        )\n",
    "                                        val_indices = np.append(\n",
    "                                            val_indices, indices_val\n",
    "                                        )\n",
    "\n",
    "                                        # the model is fit to the train split\n",
    "                                        model = regressor.fit(X_train, y_train)\n",
    "\n",
    "                                        # randomized folds are created\n",
    "                                        folds = KFold(shuffle=True, random_state=42)\n",
    "\n",
    "                                        # the cross score for the split is calculated\n",
    "                                        cross_scores.append(\n",
    "                                            np.mean(\n",
    "                                                cross_val_score(\n",
    "                                                    model, X[split], y[split], cv=folds\n",
    "                                                )\n",
    "                                            )\n",
    "                                        )\n",
    "\n",
    "                                        split_sizes.append(len(X[split]))\n",
    "\n",
    "                                        # predictions for the split are made\n",
    "                                        y_pred_train = model.predict(X_train)\n",
    "                                        y_pred_val = model.predict(X_val)\n",
    "\n",
    "                                        y_preds[indices_train] += y_pred_train\n",
    "                                        y_preds[indices_val] += y_pred_val\n",
    "\n",
    "                                    # y value is detransformed to get back to sale price\n",
    "                                    # with units of dollars\n",
    "                                    y_preds_descaled = (\n",
    "                                        y_preds * y_std + y_mean\n",
    "                                    ) * x_transform\n",
    "                                    y_true_descaled = (y * y_std + y_mean) * x_transform\n",
    "\n",
    "                                    # relevant metrics are ccalculated\n",
    "                                    train_score = r2_score(\n",
    "                                        y_preds_descaled[train_indices],\n",
    "                                        y_true_descaled[train_indices],\n",
    "                                    )\n",
    "                                    train_rmse = mean_squared_error(\n",
    "                                        y_preds_descaled[train_indices],\n",
    "                                        y_true_descaled[train_indices],\n",
    "                                        squared=False,\n",
    "                                    )\n",
    "\n",
    "                                    val_score = r2_score(\n",
    "                                        y_preds_descaled[val_indices],\n",
    "                                        y_true_descaled[val_indices],\n",
    "                                    )\n",
    "                                    val_rmse = mean_squared_error(\n",
    "                                        y_preds_descaled[val_indices],\n",
    "                                        y_true_descaled[val_indices],\n",
    "                                        squared=False,\n",
    "                                    )\n",
    "\n",
    "                                    cross_score = np.average(\n",
    "                                        cross_scores, weights=split_sizes\n",
    "                                    )\n",
    "\n",
    "                                    # information is formatted for file writing\n",
    "                                    config = [\n",
    "                                        regressor_name,\n",
    "                                        imputer_name,\n",
    "                                        feature_set_name,\n",
    "                                        area_set_name,\n",
    "                                        time_set_name,\n",
    "                                        split_name,\n",
    "                                        target_covariance_threshold,\n",
    "                                        target,\n",
    "                                    ]\n",
    "\n",
    "                                    scores = [\n",
    "                                        train_score,\n",
    "                                        val_score,\n",
    "                                        cross_score,\n",
    "                                        train_rmse,\n",
    "                                        val_rmse,\n",
    "                                    ]\n",
    "\n",
    "                                    # the best calculated cross val score is tracked for printing\n",
    "                                    # along with the rmse and config associated with that score\n",
    "                                    if cross_score > best_cross_val_score:\n",
    "                                        best_cross_val_score = cross_score\n",
    "                                        best_rmse = val_rmse\n",
    "                                        best_config = config\n",
    "\n",
    "                                    # lots of time and progress stuff is calculated for a nice printout\n",
    "                                    elapsed_time = time.time() - start_time\n",
    "\n",
    "                                    elapsed_time_tuple = str(\n",
    "                                        datetime.timedelta(seconds=elapsed_time)\n",
    "                                    ).split(\":\")\n",
    "\n",
    "                                    elapsed_time_string = f\"{elapsed_time_tuple[0]}:{elapsed_time_tuple[1]}:{round(float(elapsed_time_tuple[2])):02}\"\n",
    "\n",
    "                                    iterations_completed += 1\n",
    "\n",
    "                                    estimated_time_remaining = (\n",
    "                                        elapsed_time\n",
    "                                        * tot_iterations\n",
    "                                        / iterations_completed\n",
    "                                    ) - elapsed_time\n",
    "\n",
    "                                    estimated_remaining_time_tuple = str(\n",
    "                                        datetime.timedelta(\n",
    "                                            seconds=estimated_time_remaining\n",
    "                                        )\n",
    "                                    ).split(\":\")\n",
    "\n",
    "                                    estimated_remaining_time_string = f\"{estimated_remaining_time_tuple[0]}:{estimated_remaining_time_tuple[1]}:{round(float(estimated_remaining_time_tuple[2])):02}\"\n",
    "\n",
    "                                    to_print = (\n",
    "                                        f\"Iterations completed: {iterations_completed}/{tot_iterations}\\n\"\n",
    "                                        f\"Elapsed time: {elapsed_time_string}\\n\"\n",
    "                                        f\"Estimated time remaining: {estimated_remaining_time_string}\\n\\n\"\n",
    "                                        f\"Best cross-validation score: {round(best_cross_val_score, 5)}\\n\"\n",
    "                                        f\"Best RMSE: {round(best_rmse, 3)}\\n\\n\"\n",
    "                                        f\"Regressor: {best_config[0]}\\n\"\n",
    "                                        f\"Imputer: {best_config[1]}\\n\"\n",
    "                                        f\"Feature set: {best_config[2]}\\n\"\n",
    "                                        f\"Area set: {best_config[3]}\\n\"\n",
    "                                        f\"Time set: {best_config[4]}\\n\"\n",
    "                                        f\"Split: {best_config[5]}\\n\"\n",
    "                                        f\"Covariance threshold: {best_config[6]}\\n\"\n",
    "                                        f\"Target: {best_config[7]}\"\n",
    "                                    )\n",
    "\n",
    "                                    logline = (\n",
    "                                        \",\".join([str(s) for s in config + scores])\n",
    "                                        + \"\\n\"\n",
    "                                    )\n",
    "                                    log.write(logline)\n",
    "\n",
    "                                    clear_output(wait=True)\n",
    "                                    print(to_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b380aee9-ce7f-4a2c-917f-61aa741e5b1a",
   "metadata": {},
   "source": [
    "And with that, we have sufficient data to draw conclusions on how to design a proper model. \n",
    "\n",
    "# Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68768764-4859-40f8-ae61-7cbead140ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regressor</th>\n",
       "      <th>imputer</th>\n",
       "      <th>feature_set</th>\n",
       "      <th>area_set</th>\n",
       "      <th>time_set</th>\n",
       "      <th>split_name</th>\n",
       "      <th>covariance_threshold</th>\n",
       "      <th>target</th>\n",
       "      <th>train_score</th>\n",
       "      <th>val_score</th>\n",
       "      <th>cross_val_score</th>\n",
       "      <th>train_rmse</th>\n",
       "      <th>val_rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>lasso_0.01</td>\n",
       "      <td>mean</td>\n",
       "      <td>full</td>\n",
       "      <td>total_grade_area</td>\n",
       "      <td>full</td>\n",
       "      <td>no_split</td>\n",
       "      <td>0.0</td>\n",
       "      <td>saleprice</td>\n",
       "      <td>0.921374</td>\n",
       "      <td>0.905949</td>\n",
       "      <td>0.913029</td>\n",
       "      <td>14925.817030</td>\n",
       "      <td>16883.407446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>ridge_0.01</td>\n",
       "      <td>mean</td>\n",
       "      <td>full</td>\n",
       "      <td>total_grade_area</td>\n",
       "      <td>full</td>\n",
       "      <td>no_split</td>\n",
       "      <td>0.0</td>\n",
       "      <td>saleprice</td>\n",
       "      <td>0.921374</td>\n",
       "      <td>0.905949</td>\n",
       "      <td>0.913029</td>\n",
       "      <td>14925.817030</td>\n",
       "      <td>16883.407446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>lasso_0.01</td>\n",
       "      <td>knn</td>\n",
       "      <td>full</td>\n",
       "      <td>full</td>\n",
       "      <td>full</td>\n",
       "      <td>no_split</td>\n",
       "      <td>0.0</td>\n",
       "      <td>saleprice</td>\n",
       "      <td>0.921177</td>\n",
       "      <td>0.905692</td>\n",
       "      <td>0.913085</td>\n",
       "      <td>14954.431728</td>\n",
       "      <td>16884.491988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>ridge_0.01</td>\n",
       "      <td>knn</td>\n",
       "      <td>full</td>\n",
       "      <td>full</td>\n",
       "      <td>full</td>\n",
       "      <td>no_split</td>\n",
       "      <td>0.0</td>\n",
       "      <td>saleprice</td>\n",
       "      <td>0.921177</td>\n",
       "      <td>0.905692</td>\n",
       "      <td>0.913085</td>\n",
       "      <td>14954.431728</td>\n",
       "      <td>16884.491988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17304</th>\n",
       "      <td>ridge_0.01</td>\n",
       "      <td>mean</td>\n",
       "      <td>full</td>\n",
       "      <td>total_grade_area</td>\n",
       "      <td>year_and_month</td>\n",
       "      <td>no_split</td>\n",
       "      <td>0.0</td>\n",
       "      <td>saleprice</td>\n",
       "      <td>0.918833</td>\n",
       "      <td>0.905774</td>\n",
       "      <td>0.912698</td>\n",
       "      <td>15155.059352</td>\n",
       "      <td>16888.122382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17284</th>\n",
       "      <td>lasso_0.01</td>\n",
       "      <td>mean</td>\n",
       "      <td>full</td>\n",
       "      <td>total_grade_area</td>\n",
       "      <td>year_and_month</td>\n",
       "      <td>no_split</td>\n",
       "      <td>0.0</td>\n",
       "      <td>saleprice</td>\n",
       "      <td>0.918833</td>\n",
       "      <td>0.905774</td>\n",
       "      <td>0.912698</td>\n",
       "      <td>15155.059352</td>\n",
       "      <td>16888.122382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184</th>\n",
       "      <td>ridge_0.01</td>\n",
       "      <td>knn</td>\n",
       "      <td>full</td>\n",
       "      <td>total_grade_area</td>\n",
       "      <td>full</td>\n",
       "      <td>no_split</td>\n",
       "      <td>0.0</td>\n",
       "      <td>saleprice</td>\n",
       "      <td>0.921494</td>\n",
       "      <td>0.905650</td>\n",
       "      <td>0.912629</td>\n",
       "      <td>14917.629433</td>\n",
       "      <td>16912.340991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2164</th>\n",
       "      <td>lasso_0.01</td>\n",
       "      <td>knn</td>\n",
       "      <td>full</td>\n",
       "      <td>total_grade_area</td>\n",
       "      <td>full</td>\n",
       "      <td>no_split</td>\n",
       "      <td>0.0</td>\n",
       "      <td>saleprice</td>\n",
       "      <td>0.921494</td>\n",
       "      <td>0.905650</td>\n",
       "      <td>0.912629</td>\n",
       "      <td>14917.629433</td>\n",
       "      <td>16912.340991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22488</th>\n",
       "      <td>ridge_0.01</td>\n",
       "      <td>mean</td>\n",
       "      <td>full</td>\n",
       "      <td>total_grade_area</td>\n",
       "      <td>timestamp_and_year_month</td>\n",
       "      <td>no_split</td>\n",
       "      <td>0.0</td>\n",
       "      <td>saleprice</td>\n",
       "      <td>0.920925</td>\n",
       "      <td>0.905442</td>\n",
       "      <td>0.912633</td>\n",
       "      <td>14965.535683</td>\n",
       "      <td>16915.209837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22468</th>\n",
       "      <td>lasso_0.01</td>\n",
       "      <td>mean</td>\n",
       "      <td>full</td>\n",
       "      <td>total_grade_area</td>\n",
       "      <td>timestamp_and_year_month</td>\n",
       "      <td>no_split</td>\n",
       "      <td>0.0</td>\n",
       "      <td>saleprice</td>\n",
       "      <td>0.920925</td>\n",
       "      <td>0.905442</td>\n",
       "      <td>0.912633</td>\n",
       "      <td>14965.535683</td>\n",
       "      <td>16915.209837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17324</th>\n",
       "      <td>elasticnet_0.01_0.01</td>\n",
       "      <td>mean</td>\n",
       "      <td>full</td>\n",
       "      <td>total_grade_area</td>\n",
       "      <td>year_and_month</td>\n",
       "      <td>no_split</td>\n",
       "      <td>0.0</td>\n",
       "      <td>saleprice</td>\n",
       "      <td>0.918203</td>\n",
       "      <td>0.904940</td>\n",
       "      <td>0.912535</td>\n",
       "      <td>15176.545113</td>\n",
       "      <td>16917.982135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>elasticnet_0.01_0.01</td>\n",
       "      <td>mean</td>\n",
       "      <td>full</td>\n",
       "      <td>total_grade_area</td>\n",
       "      <td>full</td>\n",
       "      <td>no_split</td>\n",
       "      <td>0.0</td>\n",
       "      <td>saleprice</td>\n",
       "      <td>0.920765</td>\n",
       "      <td>0.905042</td>\n",
       "      <td>0.912740</td>\n",
       "      <td>14947.251597</td>\n",
       "      <td>16919.731271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>elasticnet_0.01_0.01</td>\n",
       "      <td>knn</td>\n",
       "      <td>full</td>\n",
       "      <td>full</td>\n",
       "      <td>full</td>\n",
       "      <td>no_split</td>\n",
       "      <td>0.0</td>\n",
       "      <td>saleprice</td>\n",
       "      <td>0.920657</td>\n",
       "      <td>0.904844</td>\n",
       "      <td>0.912884</td>\n",
       "      <td>14971.200847</td>\n",
       "      <td>16921.241663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ridge_0.01</td>\n",
       "      <td>mean</td>\n",
       "      <td>full</td>\n",
       "      <td>full</td>\n",
       "      <td>full</td>\n",
       "      <td>no_split</td>\n",
       "      <td>0.0</td>\n",
       "      <td>saleprice</td>\n",
       "      <td>0.920730</td>\n",
       "      <td>0.905130</td>\n",
       "      <td>0.912998</td>\n",
       "      <td>14994.319964</td>\n",
       "      <td>16928.259652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lasso_0.01</td>\n",
       "      <td>mean</td>\n",
       "      <td>full</td>\n",
       "      <td>full</td>\n",
       "      <td>full</td>\n",
       "      <td>no_split</td>\n",
       "      <td>0.0</td>\n",
       "      <td>saleprice</td>\n",
       "      <td>0.920730</td>\n",
       "      <td>0.905130</td>\n",
       "      <td>0.912998</td>\n",
       "      <td>14994.319964</td>\n",
       "      <td>16928.259652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2204</th>\n",
       "      <td>elasticnet_0.01_0.01</td>\n",
       "      <td>knn</td>\n",
       "      <td>full</td>\n",
       "      <td>total_grade_area</td>\n",
       "      <td>full</td>\n",
       "      <td>no_split</td>\n",
       "      <td>0.0</td>\n",
       "      <td>saleprice</td>\n",
       "      <td>0.920919</td>\n",
       "      <td>0.904833</td>\n",
       "      <td>0.912459</td>\n",
       "      <td>14936.078996</td>\n",
       "      <td>16941.723543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22508</th>\n",
       "      <td>elasticnet_0.01_0.01</td>\n",
       "      <td>mean</td>\n",
       "      <td>full</td>\n",
       "      <td>total_grade_area</td>\n",
       "      <td>timestamp_and_year_month</td>\n",
       "      <td>no_split</td>\n",
       "      <td>0.0</td>\n",
       "      <td>saleprice</td>\n",
       "      <td>0.920330</td>\n",
       "      <td>0.904595</td>\n",
       "      <td>0.912390</td>\n",
       "      <td>14985.213009</td>\n",
       "      <td>16944.369250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22900</th>\n",
       "      <td>lasso_0.01</td>\n",
       "      <td>knn</td>\n",
       "      <td>full</td>\n",
       "      <td>total_grade_area</td>\n",
       "      <td>timestamp_and_year_month</td>\n",
       "      <td>no_split</td>\n",
       "      <td>0.0</td>\n",
       "      <td>saleprice</td>\n",
       "      <td>0.921283</td>\n",
       "      <td>0.905156</td>\n",
       "      <td>0.912305</td>\n",
       "      <td>14935.960813</td>\n",
       "      <td>16946.679481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22920</th>\n",
       "      <td>ridge_0.01</td>\n",
       "      <td>knn</td>\n",
       "      <td>full</td>\n",
       "      <td>total_grade_area</td>\n",
       "      <td>timestamp_and_year_month</td>\n",
       "      <td>no_split</td>\n",
       "      <td>0.0</td>\n",
       "      <td>saleprice</td>\n",
       "      <td>0.921283</td>\n",
       "      <td>0.905156</td>\n",
       "      <td>0.912305</td>\n",
       "      <td>14935.960813</td>\n",
       "      <td>16946.679481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10824</th>\n",
       "      <td>ridge_0.01</td>\n",
       "      <td>knn</td>\n",
       "      <td>full</td>\n",
       "      <td>full</td>\n",
       "      <td>year_month</td>\n",
       "      <td>no_split</td>\n",
       "      <td>0.0</td>\n",
       "      <td>saleprice</td>\n",
       "      <td>0.920863</td>\n",
       "      <td>0.904795</td>\n",
       "      <td>0.912555</td>\n",
       "      <td>14982.099327</td>\n",
       "      <td>16948.780704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10804</th>\n",
       "      <td>lasso_0.01</td>\n",
       "      <td>knn</td>\n",
       "      <td>full</td>\n",
       "      <td>full</td>\n",
       "      <td>year_month</td>\n",
       "      <td>no_split</td>\n",
       "      <td>0.0</td>\n",
       "      <td>saleprice</td>\n",
       "      <td>0.920863</td>\n",
       "      <td>0.904795</td>\n",
       "      <td>0.912555</td>\n",
       "      <td>14982.099327</td>\n",
       "      <td>16948.780704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20760</th>\n",
       "      <td>ridge_0.01</td>\n",
       "      <td>mean</td>\n",
       "      <td>full</td>\n",
       "      <td>full</td>\n",
       "      <td>timestamp_and_year_month</td>\n",
       "      <td>no_split</td>\n",
       "      <td>0.0</td>\n",
       "      <td>saleprice</td>\n",
       "      <td>0.920719</td>\n",
       "      <td>0.904570</td>\n",
       "      <td>0.912746</td>\n",
       "      <td>14994.396376</td>\n",
       "      <td>16961.114070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20740</th>\n",
       "      <td>lasso_0.01</td>\n",
       "      <td>mean</td>\n",
       "      <td>full</td>\n",
       "      <td>full</td>\n",
       "      <td>timestamp_and_year_month</td>\n",
       "      <td>no_split</td>\n",
       "      <td>0.0</td>\n",
       "      <td>saleprice</td>\n",
       "      <td>0.920719</td>\n",
       "      <td>0.904570</td>\n",
       "      <td>0.912746</td>\n",
       "      <td>14994.396376</td>\n",
       "      <td>16961.114070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>elasticnet_0.01_0.01</td>\n",
       "      <td>mean</td>\n",
       "      <td>full</td>\n",
       "      <td>full</td>\n",
       "      <td>full</td>\n",
       "      <td>no_split</td>\n",
       "      <td>0.0</td>\n",
       "      <td>saleprice</td>\n",
       "      <td>0.920208</td>\n",
       "      <td>0.904285</td>\n",
       "      <td>0.912754</td>\n",
       "      <td>15010.763083</td>\n",
       "      <td>16963.013726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7368</th>\n",
       "      <td>ridge_0.01</td>\n",
       "      <td>knn</td>\n",
       "      <td>full</td>\n",
       "      <td>total_grade_area</td>\n",
       "      <td>timestamp</td>\n",
       "      <td>no_split</td>\n",
       "      <td>0.0</td>\n",
       "      <td>saleprice</td>\n",
       "      <td>0.919084</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.912307</td>\n",
       "      <td>15136.740840</td>\n",
       "      <td>16965.045357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  regressor imputer feature_set          area_set  \\\n",
       "1732             lasso_0.01    mean        full  total_grade_area   \n",
       "1752             ridge_0.01    mean        full  total_grade_area   \n",
       "436              lasso_0.01     knn        full              full   \n",
       "456              ridge_0.01     knn        full              full   \n",
       "17304            ridge_0.01    mean        full  total_grade_area   \n",
       "17284            lasso_0.01    mean        full  total_grade_area   \n",
       "2184             ridge_0.01     knn        full  total_grade_area   \n",
       "2164             lasso_0.01     knn        full  total_grade_area   \n",
       "22488            ridge_0.01    mean        full  total_grade_area   \n",
       "22468            lasso_0.01    mean        full  total_grade_area   \n",
       "17324  elasticnet_0.01_0.01    mean        full  total_grade_area   \n",
       "1772   elasticnet_0.01_0.01    mean        full  total_grade_area   \n",
       "476    elasticnet_0.01_0.01     knn        full              full   \n",
       "24               ridge_0.01    mean        full              full   \n",
       "4                lasso_0.01    mean        full              full   \n",
       "2204   elasticnet_0.01_0.01     knn        full  total_grade_area   \n",
       "22508  elasticnet_0.01_0.01    mean        full  total_grade_area   \n",
       "22900            lasso_0.01     knn        full  total_grade_area   \n",
       "22920            ridge_0.01     knn        full  total_grade_area   \n",
       "10824            ridge_0.01     knn        full              full   \n",
       "10804            lasso_0.01     knn        full              full   \n",
       "20760            ridge_0.01    mean        full              full   \n",
       "20740            lasso_0.01    mean        full              full   \n",
       "44     elasticnet_0.01_0.01    mean        full              full   \n",
       "7368             ridge_0.01     knn        full  total_grade_area   \n",
       "\n",
       "                       time_set split_name  covariance_threshold     target  \\\n",
       "1732                       full   no_split                   0.0  saleprice   \n",
       "1752                       full   no_split                   0.0  saleprice   \n",
       "436                        full   no_split                   0.0  saleprice   \n",
       "456                        full   no_split                   0.0  saleprice   \n",
       "17304            year_and_month   no_split                   0.0  saleprice   \n",
       "17284            year_and_month   no_split                   0.0  saleprice   \n",
       "2184                       full   no_split                   0.0  saleprice   \n",
       "2164                       full   no_split                   0.0  saleprice   \n",
       "22488  timestamp_and_year_month   no_split                   0.0  saleprice   \n",
       "22468  timestamp_and_year_month   no_split                   0.0  saleprice   \n",
       "17324            year_and_month   no_split                   0.0  saleprice   \n",
       "1772                       full   no_split                   0.0  saleprice   \n",
       "476                        full   no_split                   0.0  saleprice   \n",
       "24                         full   no_split                   0.0  saleprice   \n",
       "4                          full   no_split                   0.0  saleprice   \n",
       "2204                       full   no_split                   0.0  saleprice   \n",
       "22508  timestamp_and_year_month   no_split                   0.0  saleprice   \n",
       "22900  timestamp_and_year_month   no_split                   0.0  saleprice   \n",
       "22920  timestamp_and_year_month   no_split                   0.0  saleprice   \n",
       "10824                year_month   no_split                   0.0  saleprice   \n",
       "10804                year_month   no_split                   0.0  saleprice   \n",
       "20760  timestamp_and_year_month   no_split                   0.0  saleprice   \n",
       "20740  timestamp_and_year_month   no_split                   0.0  saleprice   \n",
       "44                         full   no_split                   0.0  saleprice   \n",
       "7368                  timestamp   no_split                   0.0  saleprice   \n",
       "\n",
       "       train_score  val_score  cross_val_score    train_rmse      val_rmse  \n",
       "1732      0.921374   0.905949         0.913029  14925.817030  16883.407446  \n",
       "1752      0.921374   0.905949         0.913029  14925.817030  16883.407446  \n",
       "436       0.921177   0.905692         0.913085  14954.431728  16884.491988  \n",
       "456       0.921177   0.905692         0.913085  14954.431728  16884.491988  \n",
       "17304     0.918833   0.905774         0.912698  15155.059352  16888.122382  \n",
       "17284     0.918833   0.905774         0.912698  15155.059352  16888.122382  \n",
       "2184      0.921494   0.905650         0.912629  14917.629433  16912.340991  \n",
       "2164      0.921494   0.905650         0.912629  14917.629433  16912.340991  \n",
       "22488     0.920925   0.905442         0.912633  14965.535683  16915.209837  \n",
       "22468     0.920925   0.905442         0.912633  14965.535683  16915.209837  \n",
       "17324     0.918203   0.904940         0.912535  15176.545113  16917.982135  \n",
       "1772      0.920765   0.905042         0.912740  14947.251597  16919.731271  \n",
       "476       0.920657   0.904844         0.912884  14971.200847  16921.241663  \n",
       "24        0.920730   0.905130         0.912998  14994.319964  16928.259652  \n",
       "4         0.920730   0.905130         0.912998  14994.319964  16928.259652  \n",
       "2204      0.920919   0.904833         0.912459  14936.078996  16941.723543  \n",
       "22508     0.920330   0.904595         0.912390  14985.213009  16944.369250  \n",
       "22900     0.921283   0.905156         0.912305  14935.960813  16946.679481  \n",
       "22920     0.921283   0.905156         0.912305  14935.960813  16946.679481  \n",
       "10824     0.920863   0.904795         0.912555  14982.099327  16948.780704  \n",
       "10804     0.920863   0.904795         0.912555  14982.099327  16948.780704  \n",
       "20760     0.920719   0.904570         0.912746  14994.396376  16961.114070  \n",
       "20740     0.920719   0.904570         0.912746  14994.396376  16961.114070  \n",
       "44        0.920208   0.904285         0.912754  15010.763083  16963.013726  \n",
       "7368      0.919084   0.905028         0.912307  15136.740840  16965.045357  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_stats = pd.read_csv(\"../assets/model_results_20221208170253.csv\")\n",
    "model_stats.sort_values(by=[\"val_rmse\"], ascending=True).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0571fb3-f86c-4998-9c57-4b6b0bdb9e3c",
   "metadata": {},
   "source": [
    "The output above allows me to draw a few conclusions about what the final version of the model should look like.\n",
    "\n",
    "1. Lasso, Ridge, and ElasticNet all perform comparably, and they all share the smallest value of the regularization penalty. As such, the final model will be constructed as an ElasticNet, as it is the more general of the three. Whether or not $\\lambda = 0.01$ is the correct value to use will be determined below.\n",
    "\n",
    "2. Imputing with the mean and KNN produce nearly identical results, with mean yielding slightly better RMSE values and KNN yielding slightly better cross-validation $R^2$. This difference may be an error in how the calculation of scores was handled, but it is not sufficiently large to be reason to worry. The final model will impute missing values with the mean. \n",
    "\n",
    "3. All of the features should be used, with the execption of dropping the 1st and 2nd floor areas.\n",
    "\n",
    "4. Data splitting seems to be a no-go. The exact effects of data splitting will be discussed in the results notebook, but it is clear that the most successful models did not utilize it.\n",
    "\n",
    "5. No features were filtered by target covariance. Everything that made it through other filters was included. \n",
    "\n",
    "6. Saleprice proved to be the better target to predict in the best cases.\n",
    "\n",
    "Now, the final model will be initialized and tuned for the ideal $\\lambda$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fc13f60-670d-4c33-b6ed-63ad7e35e210",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = FEATURE_SETS[\"full\"]\n",
    "time_set = TIME_SETS[\"full\"]\n",
    "area_set = AREA_SETS[\"total_grade_area\"]\n",
    "target = \"saleprice\"\n",
    "imputer = IMPUTER_VARIANTS[\"mean\"]\n",
    "\n",
    "LAMBDAS_FINE = np.linspace(0.0, 0.02, 41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93341573-270e-4637-a97b-9c935c6e0c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_train = pd.read_csv(\n",
    "    \"../datasets/train_preprocessed.csv\",\n",
    "    dtype={\"ms_subclass\": object, \"yr_sold\": object, \"mo_sold\": object},\n",
    ")\n",
    "ames_train = ames_train.reindex(sorted(ames_train.columns), axis=1)\n",
    "ames_train = z_filter(ames_train, high=Z_THRESHOLD, low=-Z_THRESHOLD)\n",
    "train_id = ames_train[\"id\"]\n",
    "ames_train = ames_train.drop(columns=[\"id\"])\n",
    "\n",
    "ames_test = pd.read_csv(\n",
    "    \"../datasets/test_preprocessed.csv\",\n",
    "    dtype={\"ms_subclass\": object, \"yr_sold\": object, \"mo_sold\": object},\n",
    ")\n",
    "ames_test = ames_test.reindex(sorted(ames_test.columns), axis=1)\n",
    "test_id = ames_test[\"id\"]\n",
    "ames_test = ames_test.drop(columns=[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5341a724-810a-4857-a62a-d81f800a96d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_training = ames_train[target]\n",
    "\n",
    "y_mean = y_training.mean()\n",
    "y_std = y_training.std()\n",
    "\n",
    "y_training = (y_training - y_mean) / y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c004ee6-e494-4961-8954-1c7346041322",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = list(np.unique(np.concatenate([feature_set, time_set, area_set])))\n",
    "\n",
    "relevant_columns = [\n",
    "    col\n",
    "    for col in ames_train.columns\n",
    "    if col not in features_to_drop and col not in TARGETS\n",
    "]\n",
    "\n",
    "X_training = ames_train.drop(columns=TARGETS)\n",
    "\n",
    "X_training = ames_train[relevant_columns]\n",
    "X_testing = ames_test[relevant_columns]\n",
    "\n",
    "# the dataframe is broken into numeric and categorical subsets\n",
    "X_numerics_training, X_objects_training = numeric_object_split(X_training)\n",
    "X_numerics_testing, X_objects_testing = numeric_object_split(X_testing)\n",
    "\n",
    "\n",
    "# numeric values are scaled\n",
    "scaler = StandardScaler()\n",
    "training_scaled = scaler.fit_transform(X_numerics_training)\n",
    "testing_scaled = scaler.transform(X_numerics_testing)\n",
    "\n",
    "\n",
    "# expected categories are declared\n",
    "categories = [X_objects_training[col].unique() for col in X_objects_training.columns]\n",
    "categories = [cat[[c != \"nan\" for c in cat]] for cat in categories]\n",
    "\n",
    "# categorical features are one-hot encoded\n",
    "ohe = OneHotEncoder(categories=categories, sparse=False, handle_unknown=\"ignore\")\n",
    "training_encoded = ohe.fit_transform(X_objects_training)\n",
    "testing_encoded = ohe.transform(X_objects_testing)\n",
    "\n",
    "\n",
    "remaining_features = np.concatenate(\n",
    "    [scaler.get_feature_names_out(), ohe.get_feature_names_out()]\n",
    ")\n",
    "\n",
    "# null values are reintroduced for proper imputation\n",
    "training_encoded = renanner(training_encoded, categories)\n",
    "testing_encoded = renanner(testing_encoded, categories)\n",
    "\n",
    "# the formatted numeric and object columns are recombined\n",
    "X_training = np.concatenate([training_scaled, training_encoded], axis=1)\n",
    "X_testing = np.concatenate([testing_scaled, testing_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf854356-f9f7-4479-9811-6f15966bb5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_imputed = imputer.fit_transform(X_training)\n",
    "testing_imputed = imputer.transform(X_testing)\n",
    "\n",
    "drop_mask = ~homogenous_columns(training_imputed)\n",
    "columns_to_drop = np.array([n for n, t in enumerate(drop_mask) if not t])\n",
    "training_imputed = training_imputed[:, drop_mask]\n",
    "testing_imputed = testing_imputed[:, drop_mask]\n",
    "remaining_features = remaining_features[drop_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e5fe616-88f6-4940-8b5f-6ec7a11fb3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part's cool\n",
    "# bivariate correlations are computed for all features\n",
    "# each feature is treated as a node on a graph\n",
    "# features with strong colinearity have an edge drawn between their nodes\n",
    "corr = np.corrcoef(training_imputed, rowvar=False)\n",
    "correlation_graph = nx.Graph()\n",
    "correlation_graph.add_nodes_from(range(corr.shape[0]))\n",
    "for row in range(corr.shape[0]):\n",
    "    high_bivariate_corr = np.where(np.abs(corr[row]) >= COVARIANCE_THRESHOLD)[0]\n",
    "    for idx in high_bivariate_corr:\n",
    "        correlation_graph.add_edge(row, idx)\n",
    "correlation_graph.remove_edges_from(nx.selfloop_edges(correlation_graph))\n",
    "\n",
    "# once the graph is completed, nodes with edges are randomly removed\n",
    "# the node is drawn from the set of nodes with the highest number of edges\n",
    "# this process is repeated until the graph is unconnected\n",
    "# this leaves only nodes with *acceptable* levels of colinearity\n",
    "# the remaining nodes are then used to index the full dataset\n",
    "max_degree = 1\n",
    "while max_degree > 0:\n",
    "    degree = correlation_graph.degree()\n",
    "    max_degree = np.max([d for n, d in degree])\n",
    "    worst_nodes = [n for n, d in degree if d == max_degree]\n",
    "    correlation_graph.remove_node(np.random.choice(worst_nodes))\n",
    "\n",
    "features_to_keep = correlation_graph.nodes\n",
    "\n",
    "X_training = training_imputed[:, features_to_keep]\n",
    "X_testing = testing_imputed[:, features_to_keep]\n",
    "remaining_features = remaining_features[features_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21b64a9f-6400-4e81-8b0a-6bf3b857fd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awalsh/.venvs/ga/lib64/python3.11/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n",
      "/home/awalsh/.venvs/ga/lib64/python3.11/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n",
      "/home/awalsh/.venvs/ga/lib64/python3.11/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n",
      "/home/awalsh/.venvs/ga/lib64/python3.11/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n"
     ]
    }
   ],
   "source": [
    "# a yeo-johnson transform is applied to the features\n",
    "yjt = PowerTransformer()\n",
    "X_training = yjt.fit_transform(X_training)\n",
    "X_testing = yjt.fit_transform(X_testing)\n",
    "yjt_parameters = yjt.lambdas_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0897856f-3b57-47a6-b643-226dcd64b4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681/1681 0.005 0.0 0.9146916958974453\n"
     ]
    }
   ],
   "source": [
    "num_its = len(LAMBDAS_FINE) ** 2\n",
    "\n",
    "best_score = 0\n",
    "best_l1 = 0\n",
    "best_l2 = 0\n",
    "i = 0\n",
    "for l1 in LAMBDAS_FINE:\n",
    "    for l2 in LAMBDAS_FINE:\n",
    "        en = ElasticNet(alpha=l1 + l2, l1_ratio=l1 / (l1 + l2))\n",
    "        folds = KFold(shuffle=True, random_state=42)\n",
    "        try:\n",
    "            score = np.mean(cross_val_score(en, X_training, y_training, cv=folds))\n",
    "        except:\n",
    "            score = -1\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_l1 = l1\n",
    "            best_l2 = l2\n",
    "        clear_output(wait=True)\n",
    "        i += 1\n",
    "        print(f\"{i}/{num_its}\", best_l1, best_l2, best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6cc99e-f2c6-4f72-acf1-f3a009a12de7",
   "metadata": {},
   "source": [
    "According to the above, the best model is a Lasso regression with $\\lambda=0.005$. Of course, now that we've eliminated the $L_2$ penalty, a hyperfine grid search can't hurt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16711d29-ebef-451d-927e-05d18c664e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001/1001 0.0048000000000000004 0.9146973415997225\n"
     ]
    }
   ],
   "source": [
    "LAMBDAS_HYPERFINE = np.linspace(0.0, 0.01, 1001)\n",
    "\n",
    "best_score = 0\n",
    "best_l1 = 0\n",
    "i = 0\n",
    "for l1 in LAMBDAS_HYPERFINE:\n",
    "    la = Lasso(alpha=l1)\n",
    "    folds = KFold(shuffle=True, random_state=42)\n",
    "    try:\n",
    "        score = np.mean(cross_val_score(la, X_training, y_training, cv=folds))\n",
    "    except:\n",
    "        score = -1\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_l1 = l1\n",
    "    clear_output(wait=True)\n",
    "    i += 1\n",
    "    print(f\"{i}/{len(LAMBDAS_HYPERFINE)}\", best_l1, best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a796df-6178-4ac3-b429-16f3c12cbeff",
   "metadata": {},
   "source": [
    "The final model used in this analysis will be a Lasso regression with $\\lambda=0.00497$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa839b4c-e8b6-454a-ab3d-7fc28e29b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "la = Lasso(alpha=0.00497)\n",
    "\n",
    "model = la.fit(X_training, y_training)\n",
    "domain_pred = la.predict(X_training)\n",
    "\n",
    "y_pred = model.predict(X_testing)\n",
    "\n",
    "betas = model.coef_\n",
    "\n",
    "y_training_descaled = y_training * y_std + y_mean\n",
    "domain_pred_descaled = domain_pred * y_std + y_mean\n",
    "y_pred_descaled = y_pred * y_std + y_mean\n",
    "\n",
    "with open(\"../assets/domain_predictions.csv\", \"w+\") as pred_file:\n",
    "    header = \"y_pred,y_true\\n\"\n",
    "    pred_file.write(header)\n",
    "    for y_p, y_t in zip(domain_pred_descaled, y_training_descaled):\n",
    "        line = f\"{y_p},{y_t}\\n\"\n",
    "        pred_file.write(line)\n",
    "\n",
    "with open(\"../assets/coefs.csv\", \"w+\") as coef_file:\n",
    "    header = \"feature,beta,lambda\\n\"\n",
    "    coef_file.write(header)\n",
    "    for n, feature in enumerate(remaining_features):\n",
    "        line = f\"{feature},{betas[n]},{yjt_parameters[n]}\\n\"\n",
    "        coef_file.write(line)\n",
    "\n",
    "\n",
    "with open(\"../datasets/submission.csv\", \"w+\") as kaggle:\n",
    "    header = \"Id,SalePrice\\n\"\n",
    "    kaggle.write(header)\n",
    "    for n, pred in enumerate(y_pred_descaled):\n",
    "        line = f\"{test_id[n]},{pred}\\n\"\n",
    "        kaggle.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af0da42-a1b7-4efa-8e24-53de05e7a6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
